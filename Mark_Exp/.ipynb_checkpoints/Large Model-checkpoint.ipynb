{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98add42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.tools import dotdict\n",
    "from driver.driver import ABC_Driver\n",
    "# from torch_geometric_temporal import METRLADatasetLoader\n",
    "# from other_model.other_model import make_default_model\n",
    "# import atd2022\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456d00af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_args = dotdict()\n",
    "\n",
    "cifar10_args.name = 'cifar10'\n",
    "cifar10_args.train_batch_size = 128\n",
    "cifar10_args.predict_batch_size = 128\n",
    "cifar10_args.device = ['cuda:0', 'cuda:1']\n",
    "\n",
    "cifar10_args.train_epochs = 250\n",
    "cifar10_args.lr = 0.01\n",
    "cifar10_args.criterion = 'CE'\n",
    "cifar10_args.optimizer = 'AdamW'\n",
    "cifar10_args.scheduler = 'OneCycle'\n",
    "cifar10_args.attack = {'fgsm':(0.005,), 'pgd':(0.005,0.1,20),'deepfool':(0.005,0.1,20),'apgd-ce':(0.005,)}\n",
    "\n",
    "activation = 'relu'\n",
    "input_channel = 3\n",
    "n=64\n",
    "knpp = [n,2*n,3*n,5*n,7*n,9*n,11*n,13*n,15*n,17*n,19*n,21*n]\n",
    "groups=n\n",
    "\n",
    "cifar10_args.layers=[\n",
    "    ('cnn2d', ((input_channel, knpp[0], (3,3), 1, 1, 1, 1), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[0], knpp[1], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[1], knpp[2], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[2], knpp[3], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[3], knpp[4], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[4], knpp[5], (3,3), 1, 1, 1, groups), 1, 'first', (2,2), activation, False)),\n",
    "    ('atrc2d', ((knpp[5], knpp[6], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[6], knpp[7], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[7], knpp[8], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[8], knpp[9], (3,3), 1, 1, 1, groups), 1, 'first', (2,2), activation, False)),\n",
    "    ('atrc2d', ((knpp[9], knpp[10], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[10], knpp[11], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('adptavgpool', (1,1)), \n",
    "    ('linear', (knpp[-1], 10, (1,2,3)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a6b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use: ['cuda:0', 'cuda:1']\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "add record: 05/09/2023 00:47\n"
     ]
    }
   ],
   "source": [
    "driver = ABC_Driver(cifar10_args, None, record_path=None, if_hash=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c908b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use: ['cuda:0', 'cuda:1']\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "add record: 05/05/2023 15:30\n",
      "epoch: 0, train_loss: 1.7397, test_metric: 0.5767, time: 1080.9909834861755\n",
      "epoch: 1, train_loss: 1.3245, test_metric: 0.6767, time: 1077.1897094249725\n",
      "epoch: 2, train_loss: 1.105, test_metric: 0.7434, time: 1077.46044921875\n",
      "epoch: 3, train_loss: 0.9854, test_metric: 0.7775, time: 1077.1588928699493\n",
      "epoch: 4, train_loss: 0.9099, test_metric: 0.7748, time: 1077.9000310897827\n",
      "epoch: 5, train_loss: 0.8371, test_metric: 0.7978, time: 1078.39155626297\n",
      "epoch: 6, train_loss: 0.7881, test_metric: 0.8227, time: 1078.2085292339325\n",
      "epoch: 7, train_loss: 0.7531, test_metric: 0.8237, time: 1078.0095698833466\n",
      "epoch: 8, train_loss: 0.7216, test_metric: 0.8276, time: 1077.7559113502502\n",
      "epoch: 9, train_loss: 0.7034, test_metric: 0.841, time: 1078.012107372284\n",
      "epoch: 10, train_loss: 0.6706, test_metric: 0.8532, time: 1077.6384491920471\n",
      "epoch: 11, train_loss: 0.6631, test_metric: 0.8424, time: 1077.6310753822327\n",
      "epoch: 12, train_loss: 0.6463, test_metric: 0.8466, time: 1078.4614560604095\n",
      "epoch: 13, train_loss: 0.6369, test_metric: 0.838, time: 1077.928382396698\n",
      "epoch: 14, train_loss: 0.6212, test_metric: 0.8579, time: 1078.7539024353027\n",
      "epoch: 15, train_loss: 0.6098, test_metric: 0.8631, time: 1077.7446601390839\n",
      "epoch: 16, train_loss: 0.5872, test_metric: 0.8554, time: 1078.1447839736938\n",
      "epoch: 17, train_loss: 0.5846, test_metric: 0.864, time: 1079.3174078464508\n",
      "epoch: 18, train_loss: 0.5791, test_metric: 0.8569, time: 1078.0331797599792\n",
      "epoch: 19, train_loss: 0.5676, test_metric: 0.8755, time: 1077.7556674480438\n",
      "epoch: 20, train_loss: 0.5587, test_metric: 0.8623, time: 1077.7824697494507\n",
      "epoch: 21, train_loss: 0.5407, test_metric: 0.8672, time: 1078.1265258789062\n",
      "epoch: 22, train_loss: 0.5322, test_metric: 0.8711, time: 1078.3362731933594\n",
      "epoch: 23, train_loss: 0.5141, test_metric: 0.8641, time: 1078.0959677696228\n",
      "epoch: 24, train_loss: 0.5056, test_metric: 0.8815, time: 1077.9647016525269\n",
      "epoch: 25, train_loss: 0.4904, test_metric: 0.8903, time: 1077.6193339824677\n",
      "epoch: 26, train_loss: 0.4807, test_metric: 0.9039, time: 1077.9664061069489\n",
      "epoch: 27, train_loss: 0.4717, test_metric: 0.8973, time: 1077.7508926391602\n",
      "epoch: 28, train_loss: 0.4475, test_metric: 0.8766, time: 1077.8302006721497\n",
      "epoch: 29, train_loss: 0.4319, test_metric: 0.9043, time: 1078.1162836551666\n",
      "epoch: 30, train_loss: 0.4122, test_metric: 0.9034, time: 1077.6427755355835\n",
      "epoch: 31, train_loss: 0.3963, test_metric: 0.8897, time: 1077.45272731781\n",
      "epoch: 32, train_loss: 0.3799, test_metric: 0.9115, time: 1077.7940783500671\n",
      "epoch: 33, train_loss: 0.3724, test_metric: 0.9085, time: 1077.7711226940155\n",
      "epoch: 34, train_loss: 0.3511, test_metric: 0.9136, time: 1078.3761348724365\n",
      "epoch: 35, train_loss: 0.3442, test_metric: 0.9101, time: 1077.7130739688873\n",
      "epoch: 36, train_loss: 0.3322, test_metric: 0.9228, time: 1077.7052483558655\n",
      "epoch: 37, train_loss: 0.323, test_metric: 0.8987, time: 1077.9627931118011\n",
      "epoch: 38, train_loss: 0.3117, test_metric: 0.9153, time: 1078.0134029388428\n",
      "epoch: 39, train_loss: 0.304, test_metric: 0.9208, time: 1078.5459139347076\n",
      "epoch: 40, train_loss: 0.291, test_metric: 0.9265, time: 1077.5876595973969\n",
      "epoch: 41, train_loss: 0.2853, test_metric: 0.9308, time: 1077.665192604065\n",
      "epoch: 42, train_loss: 0.2717, test_metric: 0.9274, time: 1078.1044027805328\n",
      "epoch: 43, train_loss: 0.2724, test_metric: 0.9272, time: 1077.8279221057892\n",
      "epoch: 44, train_loss: 0.2663, test_metric: 0.9193, time: 1077.6943764686584\n"
     ]
    }
   ],
   "source": [
    "driver = ABC_Driver(cifar10_args, None, record_path=None, if_hash=False)\n",
    "driver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525a6459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(driver.model.state_dict(), \"save/CIFAR10_ARC_2023_05_08.pt\")\n",
    "driver.model.load_state_dict(torch.load(\"save/CIFAR10_ARC_2023_05_08.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db325251",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_attack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Attention_based_CNN/Mark_Exp/driver/driver.py:79\u001b[0m, in \u001b[0;36mABC_Driver.metric\u001b[0;34m(self, test_attack)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     78\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader\u001b[38;5;241m.\u001b[39mpredict\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtargets\n\u001b[0;32m---> 79\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     80\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_true, y_pred)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_attack:\n",
      "File \u001b[0;32m~/Attention_based_CNN/Mark_Exp/driver/driver.py:71\u001b[0m, in \u001b[0;36mABC_Driver.predict\u001b[0;34m(self, pred_loader)\u001b[0m\n\u001b[1;32m     69\u001b[0m     preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([])\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, (inputs, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pred_loader):\n\u001b[0;32m---> 71\u001b[0m         pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     72\u001b[0m         preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([preds, pred], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Attention_based_CNN/Mark_Exp/model/model.py:272\u001b[0m, in \u001b[0;36mPipelineParallelABC_Net.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    269\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s_next \u001b[38;5;129;01min\u001b[39;00m splits:\n\u001b[0;32m--> 272\u001b[0m     s_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_second_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_prev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(s_prev)\n\u001b[1;32m    274\u001b[0m     s_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_first_half(s_next)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecond_half_begining_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Attention_based_CNN/Mark_Exp/model/model.py:223\u001b[0m, in \u001b[0;36mABC_Net.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_modules):\n\u001b[0;32m--> 223\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Attention_based_CNN/Mark_Exp/model/model.py:127\u001b[0m, in \u001b[0;36mConv_Module.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m    126\u001b[0m     x \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m--> 127\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayerlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mif_residual:\n\u001b[1;32m    129\u001b[0m         x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_connect(inputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Attention_based_CNN/Mark_Exp/model/ABC_Layer.py:101\u001b[0m, in \u001b[0;36mAttentionResConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_key(x)\n\u001b[1;32m    100\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munfold(query)\n\u001b[0;32m--> 101\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munfold(x)\n\u001b[1;32m    104\u001b[0m query \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_out_channels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_pixel)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/fold.py:298\u001b[0m, in \u001b[0;36mUnfold.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munfold\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py:4697\u001b[0m, in \u001b[0;36munfold\u001b[0;34m(input, kernel_size, dilation, padding, stride)\u001b[0m\n\u001b[1;32m   4693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m   4694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   4695\u001b[0m         unfold, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, kernel_size, dilation\u001b[38;5;241m=\u001b[39mdilation, padding\u001b[38;5;241m=\u001b[39mpadding, stride\u001b[38;5;241m=\u001b[39mstride\n\u001b[1;32m   4696\u001b[0m     )\n\u001b[0;32m-> 4697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim2col\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "driver.metric(test_attack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abcc047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in driver.model.module_first_half.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95091f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2549515"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24f1d80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABC_Net(\n",
       "  (full_modules): ModuleList(\n",
       "    (0): Conv_Module(\n",
       "      (layerlist): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (final_activate): ReLU()\n",
       "    )\n",
       "    (1): Conv_Module(\n",
       "      (layerlist): Sequential(\n",
       "        (0): AttentionResConv(\n",
       "          (linear_ped): Conv2d(2, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (conv_query): Conv2d(64, 576, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64, bias=False)\n",
       "          (conv_key): Conv2d(64, 576, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64, bias=False)\n",
       "          (conv_value): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (unfold): Unfold(kernel_size=(3, 3), dilation=1, padding=1, stride=1)\n",
       "          (linear): Linear(in_features=1, out_features=2, bias=True)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (final_activate): ReLU()\n",
       "    )\n",
       "    (2): Conv_Module(\n",
       "      (layerlist): Sequential(\n",
       "        (0): AttentionResConv(\n",
       "          (linear_ped): Conv2d(2, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (conv_query): Conv2d(128, 1152, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64, bias=False)\n",
       "          (conv_key): Conv2d(128, 1152, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64, bias=False)\n",
       "          (conv_value): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (unfold): Unfold(kernel_size=(3, 3), dilation=1, padding=1, stride=1)\n",
       "          (linear): Linear(in_features=2, out_features=3, bias=True)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (final_activate): ReLU()\n",
       "    )\n",
       "    (3): Conv_Module(\n",
       "      (layerlist): Sequential(\n",
       "        (0): AttentionResConv(\n",
       "          (linear_ped): Conv2d(2, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (conv_query): Conv2d(192, 1728, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64, bias=False)\n",
       "          (conv_key): Conv2d(192, 1728, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64, bias=False)\n",
       "          (conv_value): Conv2d(192, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (unfold): Unfold(kernel_size=(3, 3), dilation=1, padding=1, stride=1)\n",
       "          (linear): Linear(in_features=3, out_features=5, bias=True)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (norm): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (final_activate): ReLU()\n",
       "    )\n",
       "    (4): Conv_Module(\n",
       "      (layerlist): Sequential(\n",
       "        (0): AttentionResConv(\n",
       "          (linear_ped): Conv2d(2, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (conv_query): Conv2d(320, 2880, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64, bias=False)\n",
       "          (conv_key): Conv2d(320, 2880, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64, bias=False)\n",
       "          (conv_value): Conv2d(320, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (unfold): Unfold(kernel_size=(3, 3), dilation=1, padding=1, stride=1)\n",
       "          (linear): Linear(in_features=5, out_features=7, bias=True)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (norm): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (final_activate): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.model.module_first_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068f99b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
