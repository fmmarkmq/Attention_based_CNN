{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf43d83",
   "metadata": {},
   "source": [
    "clean:\n",
    "abc: 0.9109\n",
    "cnn: 0.9097\n",
    "\n",
    "pgd20\n",
    "alpha=0.1\n",
    "abc: 0.3411\n",
    "cnn: 0.3450\n",
    "\n",
    "alpha=1\n",
    "abc: 0.6949\n",
    "cnn: 0.6882\n",
    "\n",
    "alpha=0.15\n",
    "abc: 0.7582\n",
    "cnn: 0.7490\n",
    "\n",
    "pgd50\n",
    "abc: 0.7283\n",
    "cnn: 0.7211\n",
    "\n",
    "fgsm\n",
    "abc: 0.3871\n",
    "cnn: 0.3540\n",
    "\n",
    "\n",
    "foolbox\n",
    "fgsm: epsilon=0.031\n",
    "abc: 0.1400\n",
    "cnn: 0.2149\n",
    "\n",
    "fgsm: epsilon=0.005\n",
    "abccnn: 0.6807\n",
    "abc: 0.4621\n",
    "cnn: 0.4154\n",
    "\n",
    "pgd20: alpha=0.1 epsilon=0.005\n",
    "abccnn: 0.4328\n",
    "abc: 0.2401\n",
    "cnn: 0.0970\n",
    "\n",
    "\n",
    "ABCCNN\n",
    "clean    0.9363\n",
    "fgsm     0.6807\n",
    "pgd      0.4328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "105c2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.tools import dotdict\n",
    "from driver.driver import ABC_Driver\n",
    "# from torch_geometric_temporal import METRLADatasetLoader\n",
    "from other_model.other_model import make_default_model\n",
    "import atd2022\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab13275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_args = dotdict()\n",
    "\n",
    "cifar10_args.name = 'cifar10'\n",
    "cifar10_args.train_batch_size = 128\n",
    "cifar10_args.predict_batch_size = 128\n",
    "cifar10_args.device = ['cuda:0']\n",
    "\n",
    "cifar10_args.train_epochs = 250\n",
    "cifar10_args.lr = 0.01\n",
    "cifar10_args.criterion = 'CE'\n",
    "cifar10_args.optimizer = 'AdamW'\n",
    "cifar10_args.scheduler = 'OneCycle'\n",
    "cifar10_args.attack = {'fgsm':(0.005,), 'pgd':(0.005,0.1,20),'deepfool':(0.005,0.1,20),'apgd-ce':(0.005,)}\n",
    "\n",
    "activation = 'relu'\n",
    "input_channel = 3\n",
    "knpp = [40,80,120,160,240,320,400,480,560]\n",
    "groups=40\n",
    "\n",
    "cifar10_args.layers=[\n",
    "    ('cnn2d', ((input_channel, knpp[0], (3,3), 1, 1, 1, 1), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[0], knpp[1], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[1], knpp[2], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[2], knpp[3], (3,3), 1, 1, 1, groups), 1, 'first', (2,2), activation, False)),\n",
    "    ('atrc2d', ((knpp[3], knpp[4], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[4], knpp[5], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[5], knpp[6], (3,3), 1, 1, 1, groups), 1, 'first', (2,2), activation, False)),\n",
    "    ('atrc2d', ((knpp[6], knpp[7], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('atrc2d', ((knpp[7], knpp[8], (3,3), 1, 1, 1, groups), 1, None, None, activation, False)),\n",
    "    ('adptavgpool', (1,1)), \n",
    "    ('linear', (knpp[-1], 100, (1,2,3)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e229bf04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use: ['cuda:0']\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "add record: 03/27/2023 12:02\n",
      "epoch: 0, train_loss: 1.8453, test_metric: 0.5314, time: 731.6992020606995\n",
      "epoch: 1, train_loss: 1.4178, test_metric: 0.6197, time: 730.3269581794739\n",
      "epoch: 2, train_loss: 1.2223, test_metric: 0.65, time: 730.6451373100281\n",
      "epoch: 3, train_loss: 1.1058, test_metric: 0.7094, time: 730.5678844451904\n",
      "epoch: 4, train_loss: 1.009, test_metric: 0.6845, time: 732.0547330379486\n",
      "epoch: 5, train_loss: 0.9501, test_metric: 0.7464, time: 732.7172274589539\n",
      "epoch: 6, train_loss: 0.886, test_metric: 0.7285, time: 732.5388605594635\n",
      "epoch: 7, train_loss: 0.848, test_metric: 0.6882, time: 732.6343474388123\n",
      "epoch: 8, train_loss: 0.8114, test_metric: 0.7425, time: 732.922384262085\n",
      "epoch: 9, train_loss: 0.7717, test_metric: 0.7378, time: 733.4980013370514\n",
      "epoch: 10, train_loss: 0.7574, test_metric: 0.7839, time: 733.1797575950623\n",
      "epoch: 11, train_loss: 0.7281, test_metric: 0.7851, time: 733.5950918197632\n",
      "epoch: 12, train_loss: 0.708, test_metric: 0.7616, time: 733.095278263092\n",
      "epoch: 13, train_loss: 0.6915, test_metric: 0.7947, time: 732.91095495224\n",
      "epoch: 14, train_loss: 0.6652, test_metric: 0.8184, time: 733.3598098754883\n",
      "epoch: 15, train_loss: 0.6597, test_metric: 0.8151, time: 732.3324644565582\n",
      "epoch: 16, train_loss: 0.64, test_metric: 0.7904, time: 732.0371525287628\n",
      "epoch: 17, train_loss: 0.6237, test_metric: 0.8237, time: 731.1291573047638\n",
      "epoch: 18, train_loss: 0.6161, test_metric: 0.8253, time: 732.1896553039551\n",
      "epoch: 19, train_loss: 0.597, test_metric: 0.8354, time: 732.1788802146912\n",
      "epoch: 20, train_loss: 0.5837, test_metric: 0.8254, time: 732.0845036506653\n",
      "epoch: 21, train_loss: 0.562, test_metric: 0.8551, time: 732.1500494480133\n",
      "epoch: 22, train_loss: 0.5599, test_metric: 0.8205, time: 732.0285532474518\n",
      "epoch: 23, train_loss: 0.5447, test_metric: 0.8598, time: 731.6850144863129\n",
      "epoch: 24, train_loss: 0.5298, test_metric: 0.8429, time: 729.9261794090271\n",
      "epoch: 25, train_loss: 0.5201, test_metric: 0.8714, time: 731.7530701160431\n",
      "epoch: 26, train_loss: 0.5013, test_metric: 0.8717, time: 731.8186755180359\n",
      "epoch: 27, train_loss: 0.5025, test_metric: 0.8584, time: 732.0287487506866\n",
      "epoch: 28, train_loss: 0.4828, test_metric: 0.8637, time: 732.2623946666718\n",
      "epoch: 29, train_loss: 0.4652, test_metric: 0.8681, time: 733.6937382221222\n",
      "epoch: 30, train_loss: 0.4555, test_metric: 0.8591, time: 733.6773285865784\n",
      "epoch: 31, train_loss: 0.4399, test_metric: 0.8464, time: 733.8823754787445\n",
      "epoch: 32, train_loss: 0.4287, test_metric: 0.8743, time: 719.4007704257965\n",
      "epoch: 33, train_loss: 0.419, test_metric: 0.8694, time: 712.469484090805\n",
      "epoch: 34, train_loss: 0.4074, test_metric: 0.9004, time: 712.4899816513062\n",
      "epoch: 35, train_loss: 0.3908, test_metric: 0.8701, time: 712.5494801998138\n",
      "epoch: 36, train_loss: 0.3811, test_metric: 0.8911, time: 712.3861494064331\n",
      "epoch: 37, train_loss: 0.3719, test_metric: 0.8797, time: 712.4143886566162\n",
      "epoch: 38, train_loss: 0.3616, test_metric: 0.9015, time: 712.3079791069031\n",
      "epoch: 39, train_loss: 0.3526, test_metric: 0.8946, time: 712.3945190906525\n",
      "epoch: 40, train_loss: 0.3471, test_metric: 0.8967, time: 712.7584903240204\n",
      "epoch: 41, train_loss: 0.3371, test_metric: 0.8967, time: 713.0310242176056\n",
      "epoch: 42, train_loss: 0.3279, test_metric: 0.9088, time: 712.5752713680267\n",
      "epoch: 43, train_loss: 0.3211, test_metric: 0.8724, time: 712.5429985523224\n",
      "epoch: 44, train_loss: 0.311, test_metric: 0.9097, time: 712.4889986515045\n",
      "epoch: 45, train_loss: 0.3082, test_metric: 0.9015, time: 712.6208081245422\n",
      "epoch: 46, train_loss: 0.3007, test_metric: 0.9135, time: 713.0099730491638\n",
      "epoch: 47, train_loss: 0.2997, test_metric: 0.9133, time: 713.0383913516998\n",
      "epoch: 48, train_loss: 0.2917, test_metric: 0.9136, time: 712.8223900794983\n",
      "epoch: 49, train_loss: 0.2881, test_metric: 0.9198, time: 710.9594037532806\n",
      "epoch: 50, train_loss: 0.2779, test_metric: 0.9208, time: 710.4721450805664\n",
      "epoch: 51, train_loss: 0.2686, test_metric: 0.8877, time: 710.53564286232\n",
      "epoch: 52, train_loss: 0.2706, test_metric: 0.9151, time: 710.9855031967163\n",
      "epoch: 53, train_loss: 0.2625, test_metric: 0.9111, time: 711.5891749858856\n",
      "epoch: 54, train_loss: 0.2581, test_metric: 0.9147, time: 712.4807958602905\n",
      "epoch: 55, train_loss: 0.2559, test_metric: 0.9196, time: 712.6316096782684\n",
      "epoch: 56, train_loss: 0.2505, test_metric: 0.921, time: 712.3880422115326\n",
      "epoch: 57, train_loss: 0.2521, test_metric: 0.9106, time: 712.6306693553925\n",
      "epoch: 58, train_loss: 0.239, test_metric: 0.9086, time: 713.0564887523651\n",
      "epoch: 59, train_loss: 0.2386, test_metric: 0.915, time: 712.6870481967926\n",
      "epoch: 60, train_loss: 0.231, test_metric: 0.9193, time: 712.5288527011871\n",
      "epoch: 61, train_loss: 0.2299, test_metric: 0.9153, time: 712.6048655509949\n",
      "epoch: 62, train_loss: 0.2242, test_metric: 0.9162, time: 712.726215839386\n",
      "epoch: 63, train_loss: 0.221, test_metric: 0.9168, time: 712.9674668312073\n",
      "epoch: 64, train_loss: 0.2136, test_metric: 0.9243, time: 712.5540685653687\n",
      "epoch: 65, train_loss: 0.2112, test_metric: 0.918, time: 712.5903282165527\n",
      "epoch: 66, train_loss: 0.2062, test_metric: 0.9212, time: 712.5633590221405\n",
      "epoch: 67, train_loss: 0.2052, test_metric: 0.924, time: 712.9557201862335\n",
      "epoch: 68, train_loss: 0.1996, test_metric: 0.9236, time: 712.9823794364929\n",
      "epoch: 69, train_loss: 0.1988, test_metric: 0.9269, time: 712.5521581172943\n",
      "epoch: 70, train_loss: 0.1926, test_metric: 0.9227, time: 712.1086964607239\n",
      "epoch: 71, train_loss: 0.1902, test_metric: 0.9255, time: 710.4899523258209\n",
      "epoch: 72, train_loss: 0.1869, test_metric: 0.9241, time: 711.1332008838654\n",
      "epoch: 73, train_loss: 0.1802, test_metric: 0.9296, time: 712.428290605545\n",
      "epoch: 74, train_loss: 0.1765, test_metric: 0.9264, time: 712.4418997764587\n",
      "epoch: 75, train_loss: 0.1814, test_metric: 0.9269, time: 712.469628572464\n",
      "epoch: 76, train_loss: 0.1776, test_metric: 0.9264, time: 712.5068085193634\n",
      "epoch: 77, train_loss: 0.1716, test_metric: 0.9312, time: 712.7125263214111\n",
      "epoch: 78, train_loss: 0.1657, test_metric: 0.9271, time: 712.3928101062775\n",
      "epoch: 79, train_loss: 0.1662, test_metric: 0.9324, time: 712.2989308834076\n",
      "epoch: 80, train_loss: 0.1662, test_metric: 0.9297, time: 712.2586691379547\n",
      "epoch: 81, train_loss: 0.1586, test_metric: 0.9294, time: 712.279750585556\n",
      "epoch: 82, train_loss: 0.1596, test_metric: 0.9267, time: 712.8286554813385\n",
      "epoch: 83, train_loss: 0.1539, test_metric: 0.9277, time: 712.623664855957\n",
      "epoch: 84, train_loss: 0.1512, test_metric: 0.9316, time: 712.3918676376343\n",
      "epoch: 85, train_loss: 0.1475, test_metric: 0.9204, time: 712.317120552063\n",
      "epoch: 86, train_loss: 0.1443, test_metric: 0.931, time: 712.3944773674011\n",
      "epoch: 87, train_loss: 0.1471, test_metric: 0.9329, time: 713.3552222251892\n",
      "epoch: 88, train_loss: 0.1403, test_metric: 0.9335, time: 713.6157574653625\n",
      "epoch: 89, train_loss: 0.1456, test_metric: 0.9349, time: 713.4017550945282\n",
      "epoch: 90, train_loss: 0.1398, test_metric: 0.9365, time: 709.663907289505\n",
      "epoch: 91, train_loss: 0.1332, test_metric: 0.9261, time: 699.9112782478333\n",
      "epoch: 92, train_loss: 0.1384, test_metric: 0.934, time: 700.2318177223206\n",
      "epoch: 93, train_loss: 0.1353, test_metric: 0.9352, time: 699.8161425590515\n",
      "epoch: 94, train_loss: 0.1332, test_metric: 0.9347, time: 698.2342731952667\n",
      "epoch: 95, train_loss: 0.1266, test_metric: 0.9345, time: 697.3614382743835\n",
      "epoch: 96, train_loss: 0.1263, test_metric: 0.9368, time: 697.4061422348022\n",
      "epoch: 97, train_loss: 0.1257, test_metric: 0.94, time: 697.7281105518341\n",
      "epoch: 98, train_loss: 0.1241, test_metric: 0.9406, time: 697.4288787841797\n",
      "epoch: 99, train_loss: 0.1231, test_metric: 0.9415, time: 697.8601002693176\n",
      "epoch: 100, train_loss: 0.1158, test_metric: 0.9298, time: 698.046219587326\n",
      "epoch: 101, train_loss: 0.1198, test_metric: 0.9375, time: 697.8429458141327\n",
      "epoch: 102, train_loss: 0.1159, test_metric: 0.9364, time: 697.5183539390564\n",
      "epoch: 103, train_loss: 0.1172, test_metric: 0.942, time: 697.6019921302795\n",
      "epoch: 104, train_loss: 0.1144, test_metric: 0.9429, time: 697.8432202339172\n",
      "epoch: 105, train_loss: 0.1162, test_metric: 0.9344, time: 697.7322928905487\n",
      "epoch: 106, train_loss: 0.1105, test_metric: 0.9428, time: 697.9814479351044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 108, train_loss: 0.1079, test_metric: 0.942, time: 697.5037336349487\n",
      "epoch: 109, train_loss: 0.1029, test_metric: 0.9443, time: 697.4538731575012\n",
      "epoch: 110, train_loss: 0.1056, test_metric: 0.9407, time: 697.4034421443939\n",
      "epoch: 111, train_loss: 0.1044, test_metric: 0.9374, time: 697.4445436000824\n",
      "epoch: 112, train_loss: 0.1016, test_metric: 0.9375, time: 697.657265663147\n",
      "epoch: 113, train_loss: 0.1031, test_metric: 0.9413, time: 698.395589351654\n",
      "epoch: 114, train_loss: 0.0992, test_metric: 0.9378, time: 700.355633020401\n",
      "epoch: 115, train_loss: 0.1012, test_metric: 0.9426, time: 700.3838155269623\n",
      "epoch: 116, train_loss: 0.0991, test_metric: 0.9407, time: 700.2000677585602\n",
      "epoch: 117, train_loss: 0.0966, test_metric: 0.933, time: 700.1348671913147\n",
      "epoch: 118, train_loss: 0.0943, test_metric: 0.9423, time: 700.1909947395325\n",
      "epoch: 119, train_loss: 0.0943, test_metric: 0.9411, time: 700.1229360103607\n",
      "epoch: 120, train_loss: 0.0884, test_metric: 0.9414, time: 700.1937491893768\n",
      "epoch: 121, train_loss: 0.0936, test_metric: 0.9381, time: 700.3086974620819\n",
      "epoch: 122, train_loss: 0.0927, test_metric: 0.9425, time: 700.0581207275391\n",
      "epoch: 123, train_loss: 0.0887, test_metric: 0.9399, time: 700.0871360301971\n",
      "epoch: 124, train_loss: 0.0882, test_metric: 0.9448, time: 700.1557524204254\n",
      "epoch: 125, train_loss: 0.0882, test_metric: 0.9427, time: 700.2333357334137\n",
      "epoch: 126, train_loss: 0.0884, test_metric: 0.9398, time: 700.1045405864716\n",
      "epoch: 127, train_loss: 0.0871, test_metric: 0.9428, time: 700.1948947906494\n",
      "epoch: 128, train_loss: 0.0837, test_metric: 0.9456, time: 700.1902861595154\n",
      "epoch: 129, train_loss: 0.0827, test_metric: 0.9445, time: 700.3355224132538\n",
      "epoch: 130, train_loss: 0.078, test_metric: 0.9447, time: 700.2317411899567\n",
      "epoch: 131, train_loss: 0.0846, test_metric: 0.9411, time: 700.271252155304\n",
      "epoch: 132, train_loss: 0.0816, test_metric: 0.9417, time: 700.1780037879944\n",
      "epoch: 133, train_loss: 0.0786, test_metric: 0.9448, time: 700.2977550029755\n",
      "epoch: 134, train_loss: 0.0785, test_metric: 0.9401, time: 700.347517490387\n",
      "epoch: 135, train_loss: 0.077, test_metric: 0.9471, time: 700.2558310031891\n",
      "epoch: 136, train_loss: 0.0788, test_metric: 0.9429, time: 700.3840413093567\n",
      "epoch: 137, train_loss: 0.0777, test_metric: 0.9419, time: 700.3713130950928\n",
      "epoch: 138, train_loss: 0.0753, test_metric: 0.9459, time: 700.291348695755\n",
      "epoch: 139, train_loss: 0.0738, test_metric: 0.9445, time: 700.0987377166748\n",
      "epoch: 140, train_loss: 0.0684, test_metric: 0.9481, time: 700.1329243183136\n",
      "epoch: 141, train_loss: 0.0716, test_metric: 0.9449, time: 700.0911741256714\n",
      "epoch: 142, train_loss: 0.0705, test_metric: 0.9405, time: 700.1488926410675\n",
      "epoch: 144, train_loss: 0.0677, test_metric: 0.944, time: 700.0634276866913\n",
      "epoch: 145, train_loss: 0.0676, test_metric: 0.9474, time: 700.0978734493256\n",
      "epoch: 146, train_loss: 0.0681, test_metric: 0.944, time: 700.1271069049835\n",
      "epoch: 147, train_loss: 0.0666, test_metric: 0.9448, time: 700.0757660865784\n",
      "epoch: 148, train_loss: 0.0641, test_metric: 0.9459, time: 700.0312240123749\n",
      "epoch: 149, train_loss: 0.0668, test_metric: 0.9454, time: 700.1587529182434\n",
      "epoch: 150, train_loss: 0.0643, test_metric: 0.9491, time: 700.2776031494141\n",
      "epoch: 151, train_loss: 0.0645, test_metric: 0.9484, time: 700.1294951438904\n",
      "epoch: 152, train_loss: 0.0593, test_metric: 0.9458, time: 700.0347969532013\n",
      "epoch: 153, train_loss: 0.0603, test_metric: 0.9446, time: 700.1816182136536\n",
      "epoch: 154, train_loss: 0.0617, test_metric: 0.9454, time: 700.1509907245636\n",
      "epoch: 155, train_loss: 0.0614, test_metric: 0.9481, time: 700.1420016288757\n",
      "epoch: 156, train_loss: 0.0597, test_metric: 0.945, time: 700.2084059715271\n",
      "epoch: 157, train_loss: 0.0558, test_metric: 0.9461, time: 700.1973299980164\n",
      "epoch: 158, train_loss: 0.0564, test_metric: 0.947, time: 700.2393205165863\n",
      "epoch: 159, train_loss: 0.059, test_metric: 0.9477, time: 700.1659314632416\n",
      "epoch: 161, train_loss: 0.054, test_metric: 0.9495, time: 700.2063391208649\n",
      "epoch: 162, train_loss: 0.0544, test_metric: 0.9472, time: 700.0343449115753\n",
      "epoch: 163, train_loss: 0.054, test_metric: 0.9491, time: 700.2541036605835\n",
      "epoch: 164, train_loss: 0.0543, test_metric: 0.946, time: 700.1583566665649\n",
      "epoch: 165, train_loss: 0.0538, test_metric: 0.9484, time: 700.0600051879883\n",
      "epoch: 166, train_loss: 0.0506, test_metric: 0.9493, time: 700.121527671814\n",
      "epoch: 167, train_loss: 0.0517, test_metric: 0.9479, time: 700.1966075897217\n",
      "epoch: 168, train_loss: 0.0516, test_metric: 0.9465, time: 700.312150478363\n",
      "epoch: 169, train_loss: 0.0502, test_metric: 0.9485, time: 700.2131690979004\n",
      "epoch: 170, train_loss: 0.0502, test_metric: 0.9472, time: 700.0141041278839\n",
      "epoch: 171, train_loss: 0.0532, test_metric: 0.9484, time: 700.1941120624542\n",
      "epoch: 172, train_loss: 0.0503, test_metric: 0.9465, time: 700.0710744857788\n",
      "epoch: 173, train_loss: 0.0502, test_metric: 0.9493, time: 700.2405750751495\n",
      "epoch: 174, train_loss: 0.0473, test_metric: 0.9496, time: 700.0395359992981\n",
      "epoch: 175, train_loss: 0.0511, test_metric: 0.948, time: 700.0826640129089\n",
      "epoch: 176, train_loss: 0.0489, test_metric: 0.9475, time: 700.214476108551\n",
      "epoch: 177, train_loss: 0.0475, test_metric: 0.9465, time: 700.1588122844696\n",
      "epoch: 178, train_loss: 0.0464, test_metric: 0.9492, time: 700.2194986343384\n",
      "epoch: 179, train_loss: 0.0426, test_metric: 0.9495, time: 700.0462982654572\n",
      "epoch: 180, train_loss: 0.0444, test_metric: 0.9519, time: 700.0715284347534\n",
      "epoch: 181, train_loss: 0.0448, test_metric: 0.9511, time: 700.0232157707214\n",
      "epoch: 182, train_loss: 0.0462, test_metric: 0.9517, time: 699.8668265342712\n",
      "epoch: 183, train_loss: 0.0433, test_metric: 0.9524, time: 700.0324397087097\n",
      "epoch: 184, train_loss: 0.045, test_metric: 0.9505, time: 700.0560145378113\n",
      "epoch: 185, train_loss: 0.0431, test_metric: 0.9508, time: 700.1478765010834\n",
      "epoch: 186, train_loss: 0.0424, test_metric: 0.9498, time: 700.2443289756775\n",
      "epoch: 187, train_loss: 0.0403, test_metric: 0.9505, time: 700.3519592285156\n",
      "epoch: 188, train_loss: 0.043, test_metric: 0.9502, time: 700.2267305850983\n",
      "epoch: 189, train_loss: 0.0401, test_metric: 0.9522, time: 700.3531002998352\n",
      "epoch: 190, train_loss: 0.041, test_metric: 0.9511, time: 700.314001083374\n",
      "epoch: 191, train_loss: 0.038, test_metric: 0.9535, time: 700.3932263851166\n",
      "epoch: 192, train_loss: 0.0411, test_metric: 0.9512, time: 700.2781422138214\n",
      "epoch: 193, train_loss: 0.0425, test_metric: 0.9499, time: 700.2187304496765\n",
      "epoch: 194, train_loss: 0.0391, test_metric: 0.9496, time: 700.276529788971\n",
      "epoch: 195, train_loss: 0.0411, test_metric: 0.9509, time: 700.266649723053\n",
      "epoch: 196, train_loss: 0.0376, test_metric: 0.9523, time: 700.2976903915405\n",
      "epoch: 197, train_loss: 0.04, test_metric: 0.9509, time: 700.3695702552795\n",
      "epoch: 198, train_loss: 0.0384, test_metric: 0.9513, time: 700.0789189338684\n",
      "epoch: 199, train_loss: 0.0367, test_metric: 0.9508, time: 700.0976264476776\n",
      "epoch: 200, train_loss: 0.0362, test_metric: 0.9501, time: 700.2277064323425\n",
      "epoch: 201, train_loss: 0.0365, test_metric: 0.9506, time: 700.1225106716156\n",
      "epoch: 202, train_loss: 0.0363, test_metric: 0.9513, time: 700.1290431022644\n",
      "epoch: 203, train_loss: 0.0376, test_metric: 0.9521, time: 700.1164445877075\n",
      "epoch: 204, train_loss: 0.0366, test_metric: 0.9527, time: 700.0892374515533\n",
      "epoch: 205, train_loss: 0.0355, test_metric: 0.9504, time: 700.2811353206635\n",
      "epoch: 206, train_loss: 0.0361, test_metric: 0.9522, time: 700.2078287601471\n",
      "epoch: 207, train_loss: 0.0354, test_metric: 0.9536, time: 700.1094419956207\n",
      "epoch: 208, train_loss: 0.0341, test_metric: 0.9521, time: 700.2529685497284\n",
      "epoch: 209, train_loss: 0.0364, test_metric: 0.9538, time: 700.3006558418274\n",
      "epoch: 210, train_loss: 0.0326, test_metric: 0.9551, time: 700.1441349983215\n",
      "epoch: 211, train_loss: 0.0322, test_metric: 0.9539, time: 700.141304731369\n",
      "epoch: 212, train_loss: 0.0334, test_metric: 0.9541, time: 700.0879623889923\n",
      "epoch: 213, train_loss: 0.0352, test_metric: 0.9542, time: 700.2806725502014\n",
      "epoch: 214, train_loss: 0.0337, test_metric: 0.954, time: 700.2171771526337\n",
      "epoch: 215, train_loss: 0.0341, test_metric: 0.9548, time: 700.0575053691864\n",
      "epoch: 216, train_loss: 0.034, test_metric: 0.953, time: 700.0456681251526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 217, train_loss: 0.0329, test_metric: 0.9536, time: 700.005752325058\n",
      "epoch: 218, train_loss: 0.0335, test_metric: 0.9539, time: 700.2168321609497\n",
      "epoch: 219, train_loss: 0.0317, test_metric: 0.9539, time: 700.1229050159454\n",
      "epoch: 220, train_loss: 0.0315, test_metric: 0.954, time: 700.0949003696442\n",
      "epoch: 221, train_loss: 0.0314, test_metric: 0.9532, time: 700.1822876930237\n",
      "epoch: 222, train_loss: 0.0324, test_metric: 0.9527, time: 700.1228122711182\n",
      "epoch: 223, train_loss: 0.0328, test_metric: 0.9533, time: 700.0319769382477\n",
      "epoch: 224, train_loss: 0.0317, test_metric: 0.9522, time: 700.2214684486389\n",
      "epoch: 225, train_loss: 0.033, test_metric: 0.9539, time: 700.0872621536255\n",
      "epoch: 226, train_loss: 0.0315, test_metric: 0.9542, time: 700.084091424942\n",
      "epoch: 227, train_loss: 0.0327, test_metric: 0.9533, time: 700.2473497390747\n",
      "epoch: 228, train_loss: 0.0316, test_metric: 0.9538, time: 699.9895298480988\n",
      "epoch: 229, train_loss: 0.031, test_metric: 0.9532, time: 700.0467731952667\n",
      "epoch: 230, train_loss: 0.0322, test_metric: 0.9535, time: 700.1590864658356\n",
      "epoch: 231, train_loss: 0.03, test_metric: 0.9541, time: 700.2148070335388\n",
      "epoch: 232, train_loss: 0.0313, test_metric: 0.9545, time: 700.0096242427826\n",
      "epoch: 233, train_loss: 0.0304, test_metric: 0.9555, time: 699.8412351608276\n",
      "epoch: 234, train_loss: 0.0309, test_metric: 0.9537, time: 699.9700059890747\n",
      "epoch: 235, train_loss: 0.03, test_metric: 0.9538, time: 700.399246931076\n",
      "epoch: 236, train_loss: 0.0302, test_metric: 0.9548, time: 699.982498884201\n",
      "epoch: 237, train_loss: 0.0304, test_metric: 0.9544, time: 699.1129643917084\n",
      "epoch: 238, train_loss: 0.0318, test_metric: 0.9542, time: 698.9301261901855\n",
      "epoch: 239, train_loss: 0.0306, test_metric: 0.9537, time: 699.4179589748383\n",
      "epoch: 240, train_loss: 0.032, test_metric: 0.9538, time: 700.0442070960999\n",
      "epoch: 241, train_loss: 0.0299, test_metric: 0.954, time: 699.7794020175934\n",
      "epoch: 242, train_loss: 0.0301, test_metric: 0.9527, time: 700.0481464862823\n",
      "epoch: 243, train_loss: 0.033, test_metric: 0.9525, time: 700.1142997741699\n",
      "epoch: 244, train_loss: 0.0311, test_metric: 0.9529, time: 699.9218504428864\n",
      "epoch: 245, train_loss: 0.0316, test_metric: 0.9542, time: 699.9516718387604\n",
      "epoch: 246, train_loss: 0.0322, test_metric: 0.9536, time: 699.8493797779083\n",
      "epoch: 247, train_loss: 0.0304, test_metric: 0.9543, time: 699.9633078575134\n",
      "epoch: 248, train_loss: 0.0309, test_metric: 0.9531, time: 699.7965927124023\n",
      "epoch: 249, train_loss: 0.032, test_metric: 0.9543, time: 700.0106792449951\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.56 GiB (GPU 0; 47.46 GiB total capacity; 30.77 GiB already allocated; 2.06 GiB free; 44.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m driver \u001b[38;5;241m=\u001b[39m ABC_Driver(cifar10_args, \u001b[38;5;28;01mNone\u001b[39;00m, record_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, if_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/mfeng/Attention_based_CNN/Mark_Exp/driver/driver.py:59\u001b[0m, in \u001b[0;36mABC_Driver.train\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m     57\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord\u001b[38;5;241m.\u001b[39madd_train_log(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(epoch), np\u001b[38;5;241m.\u001b[39maverage(train_loss), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric(), if_print\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord\u001b[38;5;241m.\u001b[39madd_test_outcome(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_attack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/scratch/mfeng/Attention_based_CNN/Mark_Exp/driver/driver.py:84\u001b[0m, in \u001b[0;36mABC_Driver.metric\u001b[0;34m(self, test_attack)\u001b[0m\n\u001b[1;32m     82\u001b[0m     metric \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m'\u001b[39m:accuracy})\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attacker_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mattack:\n\u001b[0;32m---> 84\u001b[0m         metric[attacker_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattacker_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metric\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/mfeng/Attention_based_CNN/Mark_Exp/attack/attack.py:24\u001b[0m, in \u001b[0;36mAttack.__call__\u001b[0;34m(self, attacker_name, model)\u001b[0m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attacker_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfgsm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpgd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeepfool\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfb_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattacker_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attacker_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapgd-ce\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_attack(attacker_name, model)\n",
      "File \u001b[0;32m/scratch/mfeng/Attention_based_CNN/Mark_Exp/attack/attack.py:45\u001b[0m, in \u001b[0;36mAttack.fb_attack\u001b[0;34m(self, attacker_name, model)\u001b[0m\n\u001b[1;32m     43\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     44\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 45\u001b[0m     _, _, is_adv \u001b[38;5;241m=\u001b[39m \u001b[43mattacker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     suc_all \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([suc_all, is_adv])\n\u001b[1;32m     47\u001b[0m attack_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m suc_all\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/foolbox/attacks/base.py:283\u001b[0m, in \u001b[0;36mFixedEpsilonAttack.__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    281\u001b[0m success \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epsilon \u001b[38;5;129;01min\u001b[39;00m real_epsilons:\n\u001b[0;32m--> 283\u001b[0m     xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# clip to epsilon because we don't really know what the attack returns;\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# alternatively, we could check if the perturbation is at most epsilon,\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# but then we would need to handle numerical violations;\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     xpc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance\u001b[38;5;241m.\u001b[39mclip_perturbation(x, xp, epsilon)\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/foolbox/attacks/fast_gradient_method.py:98\u001b[0m, in \u001b[0;36mLinfFastGradientAttack.run\u001b[0;34m(self, model, inputs, criterion, epsilon, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(criterion, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported criterion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/foolbox/attacks/gradient_descent_base.py:155\u001b[0m, in \u001b[0;36mBaseGradientDescent.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    152\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m--> 155\u001b[0m     _, gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(gradients, x\u001b[38;5;241m=\u001b[39mx, bounds\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbounds)\n\u001b[1;32m    157\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m gradient_step_sign \u001b[38;5;241m*\u001b[39m optimizer(gradients)\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/foolbox/attacks/gradient_descent_base.py:111\u001b[0m, in \u001b[0;36mBaseGradientDescent.value_and_grad\u001b[0;34m(self, loss_fn, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue_and_grad\u001b[39m(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# can be overridden by users\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    108\u001b[0m     loss_fn: Callable[[ep\u001b[38;5;241m.\u001b[39mTensor], ep\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    109\u001b[0m     x: ep\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    110\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[ep\u001b[38;5;241m.\u001b[39mTensor, ep\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/eagerpy/framework.py:360\u001b[0m, in \u001b[0;36mvalue_and_grad\u001b[0;34m(f, t, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue_and_grad\u001b[39m(\n\u001b[1;32m    358\u001b[0m     f: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, TensorType], t: TensorType, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    359\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TensorType, TensorType]:\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/eagerpy/tensor/tensor.py:553\u001b[0m, in \u001b[0;36mTensor.value_and_grad\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue_and_grad\u001b[39m(\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m: TensorType, f: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, TensorType], \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    552\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TensorType, TensorType]:\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value_and_grad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/eagerpy/tensor/pytorch.py:505\u001b[0m, in \u001b[0;36mPyTorchTensor._value_and_grad_fn.<locals>.value_and_grad\u001b[0;34m(x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m     loss, aux \u001b[38;5;241m=\u001b[39m f(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mraw\n\u001b[1;32m    507\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/foolbox/attacks/gradient_descent_base.py:96\u001b[0m, in \u001b[0;36mBaseGradientDescent.get_loss_fn.<locals>.loss_fn\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(inputs: ep\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ep\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 96\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ep\u001b[38;5;241m.\u001b[39mcrossentropy(logits, labels)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/foolbox/models/base.py:102\u001b[0m, in \u001b[0;36mModelWithPreprocessing.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    100\u001b[0m x, restore_type \u001b[38;5;241m=\u001b[39m ep\u001b[38;5;241m.\u001b[39mastensor_(inputs)\n\u001b[1;32m    101\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess(x)\n\u001b[0;32m--> 102\u001b[0m z \u001b[38;5;241m=\u001b[39m ep\u001b[38;5;241m.\u001b[39mastensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restore_type(z)\n",
      "File \u001b[0;32m/scratch/mfeng/Attention_based_CNN/Mark_Exp/attack/fb_PyTorchModel_revised.py:50\u001b[0m, in \u001b[0;36mPyTorchModel.__init__.<locals>._model\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_model\u001b[39m(x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(x\u001b[38;5;241m.\u001b[39mrequires_grad):\n\u001b[0;32m---> 50\u001b[0m         result \u001b[38;5;241m=\u001b[39m cast(torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/mfeng/Attention_based_CNN/Mark_Exp/model/model.py:223\u001b[0m, in \u001b[0;36mABC_Net.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_modules):\n\u001b[0;32m--> 223\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/mfeng/Attention_based_CNN/Mark_Exp/model/model.py:127\u001b[0m, in \u001b[0;36mConv_Module.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m    126\u001b[0m     x \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m--> 127\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayerlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mif_residual:\n\u001b[1;32m    129\u001b[0m         x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_connect(inputs)\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/mfeng/Attention_based_CNN/Mark_Exp/model/ABC_Layer.py:112\u001b[0m, in \u001b[0;36mAttentionResConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    110\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_pixel)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m att \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m att \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_ped)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_len)\n\u001b[1;32m    114\u001b[0m att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(att)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.56 GiB (GPU 0; 47.46 GiB total capacity; 30.77 GiB already allocated; 2.06 GiB free; 44.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "driver = ABC_Driver(cifar10_args, None, record_path=None, if_hash=False)\n",
    "driver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f0e50b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a787603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2813cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use: ['cuda:0']\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "add record: 03/25/2023 16:31\n",
      "epoch: 0, train_loss: 1.7716, test_metric: 0.4988, time: 593.2482035160065\n",
      "epoch: 1, train_loss: 1.4845, test_metric: 0.5628, time: 591.7711906433105\n",
      "epoch: 2, train_loss: 1.3432, test_metric: 0.6113, time: 591.7671165466309\n",
      "epoch: 3, train_loss: 1.2384, test_metric: 0.6279, time: 591.7860770225525\n",
      "epoch: 4, train_loss: 1.1637, test_metric: 0.6348, time: 591.573098897934\n",
      "epoch: 5, train_loss: 1.0978, test_metric: 0.6952, time: 591.6569447517395\n",
      "epoch: 6, train_loss: 1.0395, test_metric: 0.7138, time: 592.107753276825\n",
      "epoch: 7, train_loss: 1.0017, test_metric: 0.7064, time: 592.1148927211761\n",
      "epoch: 8, train_loss: 0.9595, test_metric: 0.7341, time: 591.9846529960632\n",
      "epoch: 9, train_loss: 0.9299, test_metric: 0.7449, time: 591.9319047927856\n",
      "epoch: 10, train_loss: 0.8987, test_metric: 0.745, time: 591.9915819168091\n",
      "epoch: 11, train_loss: 0.8715, test_metric: 0.7551, time: 591.622964143753\n",
      "epoch: 12, train_loss: 0.8427, test_metric: 0.7659, time: 591.6776993274689\n",
      "epoch: 13, train_loss: 0.8269, test_metric: 0.7608, time: 591.9147951602936\n",
      "epoch: 14, train_loss: 0.7924, test_metric: 0.739, time: 592.1581139564514\n",
      "epoch: 15, train_loss: 0.7903, test_metric: 0.7626, time: 591.8559060096741\n",
      "epoch: 16, train_loss: 0.7754, test_metric: 0.772, time: 591.954820394516\n",
      "epoch: 17, train_loss: 0.7551, test_metric: 0.8014, time: 591.388022184372\n",
      "epoch: 18, train_loss: 0.7314, test_metric: 0.8197, time: 591.8515450954437\n",
      "epoch: 19, train_loss: 0.709, test_metric: 0.7971, time: 591.7210624217987\n",
      "epoch: 20, train_loss: 0.6972, test_metric: 0.81, time: 591.8080124855042\n",
      "epoch: 21, train_loss: 0.6811, test_metric: 0.7859, time: 591.6885919570923\n",
      "epoch: 22, train_loss: 0.6595, test_metric: 0.7889, time: 592.4101102352142\n",
      "epoch: 23, train_loss: 0.6479, test_metric: 0.8172, time: 592.4966588020325\n",
      "epoch: 24, train_loss: 0.6285, test_metric: 0.8382, time: 592.399561882019\n",
      "epoch: 25, train_loss: 0.6182, test_metric: 0.832, time: 592.3217713832855\n",
      "epoch: 26, train_loss: 0.5999, test_metric: 0.8, time: 592.3264117240906\n",
      "epoch: 27, train_loss: 0.5774, test_metric: 0.8226, time: 592.2208840847015\n",
      "epoch: 28, train_loss: 0.5662, test_metric: 0.8158, time: 592.4472692012787\n",
      "epoch: 29, train_loss: 0.5561, test_metric: 0.8458, time: 592.2511155605316\n",
      "epoch: 30, train_loss: 0.5455, test_metric: 0.8233, time: 592.4488749504089\n",
      "epoch: 31, train_loss: 0.5245, test_metric: 0.8351, time: 592.4392786026001\n",
      "epoch: 32, train_loss: 0.5095, test_metric: 0.8664, time: 592.4566698074341\n",
      "epoch: 33, train_loss: 0.4908, test_metric: 0.8508, time: 592.2256321907043\n",
      "epoch: 34, train_loss: 0.4746, test_metric: 0.857, time: 592.4556272029877\n",
      "epoch: 35, train_loss: 0.4583, test_metric: 0.8446, time: 592.320864200592\n",
      "epoch: 36, train_loss: 0.4419, test_metric: 0.8557, time: 592.6886758804321\n",
      "epoch: 37, train_loss: 0.4321, test_metric: 0.8703, time: 592.5680930614471\n",
      "epoch: 38, train_loss: 0.4171, test_metric: 0.8924, time: 592.4556193351746\n",
      "epoch: 39, train_loss: 0.4102, test_metric: 0.8828, time: 592.5195748806\n",
      "epoch: 40, train_loss: 0.3961, test_metric: 0.8976, time: 580.8775153160095\n",
      "epoch: 41, train_loss: 0.3818, test_metric: 0.889, time: 589.9612202644348\n",
      "epoch: 42, train_loss: 0.3732, test_metric: 0.8865, time: 591.7010533809662\n",
      "epoch: 43, train_loss: 0.367, test_metric: 0.8995, time: 591.419753074646\n",
      "epoch: 44, train_loss: 0.3621, test_metric: 0.8993, time: 591.6825942993164\n",
      "epoch: 45, train_loss: 0.3456, test_metric: 0.902, time: 591.6270127296448\n",
      "epoch: 46, train_loss: 0.3436, test_metric: 0.8941, time: 592.0168299674988\n",
      "epoch: 47, train_loss: 0.3368, test_metric: 0.8998, time: 591.6768827438354\n",
      "epoch: 48, train_loss: 0.3307, test_metric: 0.8941, time: 591.5037503242493\n",
      "epoch: 49, train_loss: 0.3148, test_metric: 0.9121, time: 591.5098767280579\n",
      "epoch: 50, train_loss: 0.3129, test_metric: 0.8961, time: 591.4465856552124\n",
      "epoch: 51, train_loss: 0.3063, test_metric: 0.9063, time: 592.7704591751099\n",
      "epoch: 52, train_loss: 0.3032, test_metric: 0.9043, time: 592.824077129364\n",
      "epoch: 53, train_loss: 0.296, test_metric: 0.9198, time: 593.0219650268555\n",
      "epoch: 54, train_loss: 0.2887, test_metric: 0.9124, time: 593.0020024776459\n",
      "epoch: 55, train_loss: 0.2807, test_metric: 0.9148, time: 592.8492403030396\n",
      "epoch: 56, train_loss: 0.2775, test_metric: 0.9122, time: 592.8809330463409\n",
      "epoch: 57, train_loss: 0.2727, test_metric: 0.9092, time: 592.8653013706207\n",
      "epoch: 58, train_loss: 0.2674, test_metric: 0.8841, time: 592.9076542854309\n",
      "epoch: 59, train_loss: 0.2656, test_metric: 0.8973, time: 592.7315347194672\n",
      "epoch: 60, train_loss: 0.2514, test_metric: 0.9122, time: 592.8977880477905\n",
      "epoch: 61, train_loss: 0.248, test_metric: 0.9063, time: 592.7104215621948\n",
      "epoch: 62, train_loss: 0.2487, test_metric: 0.9152, time: 592.8084962368011\n",
      "epoch: 63, train_loss: 0.2435, test_metric: 0.9149, time: 592.9073560237885\n",
      "epoch: 64, train_loss: 0.2376, test_metric: 0.9184, time: 593.2077932357788\n",
      "epoch: 65, train_loss: 0.2333, test_metric: 0.9179, time: 592.6699171066284\n",
      "epoch: 66, train_loss: 0.2324, test_metric: 0.919, time: 593.0157544612885\n",
      "epoch: 67, train_loss: 0.2215, test_metric: 0.9183, time: 592.7380244731903\n",
      "epoch: 68, train_loss: 0.2262, test_metric: 0.9158, time: 592.9899821281433\n",
      "epoch: 69, train_loss: 0.221, test_metric: 0.9202, time: 593.0527634620667\n",
      "epoch: 70, train_loss: 0.2144, test_metric: 0.9172, time: 592.7543811798096\n",
      "epoch: 71, train_loss: 0.211, test_metric: 0.9154, time: 592.8212373256683\n",
      "epoch: 72, train_loss: 0.2065, test_metric: 0.9255, time: 593.1998467445374\n",
      "epoch: 73, train_loss: 0.1995, test_metric: 0.915, time: 592.9433915615082\n",
      "epoch: 74, train_loss: 0.2014, test_metric: 0.9157, time: 592.8481192588806\n",
      "epoch: 75, train_loss: 0.1965, test_metric: 0.924, time: 592.9086084365845\n",
      "epoch: 76, train_loss: 0.1846, test_metric: 0.9273, time: 592.9414746761322\n",
      "epoch: 77, train_loss: 0.19, test_metric: 0.9181, time: 592.9083671569824\n",
      "epoch: 78, train_loss: 0.1859, test_metric: 0.9241, time: 592.93315076828\n",
      "epoch: 79, train_loss: 0.1794, test_metric: 0.9291, time: 592.7503836154938\n",
      "epoch: 80, train_loss: 0.1751, test_metric: 0.9311, time: 592.898600101471\n",
      "epoch: 81, train_loss: 0.1755, test_metric: 0.9277, time: 592.7989809513092\n",
      "epoch: 82, train_loss: 0.169, test_metric: 0.9262, time: 593.2371644973755\n",
      "epoch: 83, train_loss: 0.1687, test_metric: 0.931, time: 592.6818089485168\n",
      "epoch: 84, train_loss: 0.1703, test_metric: 0.9313, time: 592.9726753234863\n",
      "epoch: 85, train_loss: 0.1575, test_metric: 0.925, time: 592.8753912448883\n",
      "epoch: 86, train_loss: 0.1666, test_metric: 0.9263, time: 592.8962407112122\n",
      "epoch: 87, train_loss: 0.1534, test_metric: 0.9313, time: 592.969973564148\n",
      "epoch: 88, train_loss: 0.1543, test_metric: 0.9295, time: 592.8364706039429\n",
      "epoch: 89, train_loss: 0.1546, test_metric: 0.9288, time: 592.7665333747864\n",
      "epoch: 90, train_loss: 0.1568, test_metric: 0.9248, time: 592.8500411510468\n",
      "epoch: 91, train_loss: 0.1462, test_metric: 0.9292, time: 592.6900334358215\n",
      "epoch: 92, train_loss: 0.1512, test_metric: 0.9363, time: 592.9317367076874\n",
      "epoch: 93, train_loss: 0.1441, test_metric: 0.9343, time: 592.9555387496948\n",
      "epoch: 94, train_loss: 0.1412, test_metric: 0.9349, time: 593.035010099411\n",
      "epoch: 95, train_loss: 0.1401, test_metric: 0.9247, time: 593.0568861961365\n",
      "epoch: 96, train_loss: 0.1365, test_metric: 0.9374, time: 592.8900525569916\n",
      "epoch: 97, train_loss: 0.1383, test_metric: 0.9346, time: 592.6564073562622\n",
      "epoch: 98, train_loss: 0.1353, test_metric: 0.9375, time: 592.8970494270325\n",
      "epoch: 99, train_loss: 0.1322, test_metric: 0.9365, time: 592.68408370018\n",
      "epoch: 100, train_loss: 0.1272, test_metric: 0.9328, time: 592.4088876247406\n",
      "epoch: 101, train_loss: 0.1301, test_metric: 0.9365, time: 592.671139717102\n",
      "epoch: 102, train_loss: 0.1349, test_metric: 0.9273, time: 592.6136198043823\n",
      "epoch: 103, train_loss: 0.1253, test_metric: 0.9344, time: 592.9689619541168\n",
      "epoch: 104, train_loss: 0.1228, test_metric: 0.9282, time: 593.0476791858673\n",
      "epoch: 105, train_loss: 0.1223, test_metric: 0.9294, time: 592.3909041881561\n",
      "epoch: 106, train_loss: 0.1191, test_metric: 0.9374, time: 592.7927734851837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 107, train_loss: 0.1207, test_metric: 0.9359, time: 592.8108556270599\n",
      "epoch: 108, train_loss: 0.1132, test_metric: 0.936, time: 592.8189835548401\n",
      "epoch: 109, train_loss: 0.1181, test_metric: 0.937, time: 592.642333984375\n",
      "epoch: 110, train_loss: 0.1155, test_metric: 0.9385, time: 593.0039255619049\n",
      "epoch: 111, train_loss: 0.1119, test_metric: 0.9386, time: 593.1877944469452\n",
      "epoch: 112, train_loss: 0.1105, test_metric: 0.9365, time: 593.3193695545197\n",
      "epoch: 113, train_loss: 0.1085, test_metric: 0.9378, time: 592.9271106719971\n",
      "epoch: 114, train_loss: 0.1099, test_metric: 0.9408, time: 592.958417892456\n",
      "epoch: 115, train_loss: 0.1069, test_metric: 0.9383, time: 592.8823490142822\n",
      "epoch: 116, train_loss: 0.1064, test_metric: 0.9447, time: 592.7913644313812\n",
      "epoch: 117, train_loss: 0.1075, test_metric: 0.9388, time: 592.788667678833\n",
      "epoch: 118, train_loss: 0.1008, test_metric: 0.9354, time: 592.8589706420898\n",
      "epoch: 119, train_loss: 0.1001, test_metric: 0.9374, time: 592.8055758476257\n",
      "epoch: 120, train_loss: 0.0997, test_metric: 0.9366, time: 593.0653822422028\n",
      "epoch: 121, train_loss: 0.1017, test_metric: 0.939, time: 592.3732488155365\n",
      "epoch: 122, train_loss: 0.0967, test_metric: 0.9361, time: 592.8746910095215\n",
      "epoch: 123, train_loss: 0.0977, test_metric: 0.9414, time: 593.1274108886719\n",
      "epoch: 124, train_loss: 0.0942, test_metric: 0.9364, time: 593.3167324066162\n",
      "epoch: 125, train_loss: 0.0982, test_metric: 0.9426, time: 592.8358678817749\n",
      "epoch: 126, train_loss: 0.0964, test_metric: 0.9386, time: 592.9601383209229\n",
      "epoch: 127, train_loss: 0.093, test_metric: 0.9403, time: 592.9947195053101\n",
      "epoch: 128, train_loss: 0.0933, test_metric: 0.9419, time: 592.9289872646332\n",
      "epoch: 129, train_loss: 0.091, test_metric: 0.942, time: 592.7390627861023\n",
      "epoch: 130, train_loss: 0.0877, test_metric: 0.9399, time: 593.0505595207214\n",
      "epoch: 131, train_loss: 0.0856, test_metric: 0.9393, time: 592.7075545787811\n",
      "epoch: 132, train_loss: 0.0908, test_metric: 0.9365, time: 592.356635093689\n",
      "epoch: 133, train_loss: 0.0856, test_metric: 0.9405, time: 592.8900032043457\n",
      "epoch: 135, train_loss: 0.0836, test_metric: 0.9401, time: 592.895587682724\n",
      "epoch: 136, train_loss: 0.0814, test_metric: 0.938, time: 592.8634076118469\n",
      "epoch: 137, train_loss: 0.0815, test_metric: 0.9408, time: 592.8004369735718\n",
      "epoch: 138, train_loss: 0.0803, test_metric: 0.9424, time: 592.9005959033966\n",
      "epoch: 139, train_loss: 0.0798, test_metric: 0.9454, time: 592.8571376800537\n",
      "epoch: 140, train_loss: 0.0779, test_metric: 0.9453, time: 593.3094756603241\n",
      "epoch: 141, train_loss: 0.0759, test_metric: 0.9398, time: 592.9297072887421\n",
      "epoch: 142, train_loss: 0.078, test_metric: 0.9398, time: 592.904013633728\n",
      "epoch: 143, train_loss: 0.0736, test_metric: 0.944, time: 592.7964715957642\n",
      "epoch: 144, train_loss: 0.0777, test_metric: 0.9429, time: 592.8598062992096\n",
      "epoch: 145, train_loss: 0.0745, test_metric: 0.9392, time: 592.7077493667603\n",
      "epoch: 146, train_loss: 0.0749, test_metric: 0.9424, time: 592.8938386440277\n",
      "epoch: 147, train_loss: 0.069, test_metric: 0.9407, time: 593.3026523590088\n",
      "epoch: 148, train_loss: 0.0717, test_metric: 0.9436, time: 593.0801753997803\n",
      "epoch: 149, train_loss: 0.0692, test_metric: 0.9425, time: 592.7867381572723\n",
      "epoch: 150, train_loss: 0.0718, test_metric: 0.9422, time: 593.0264530181885\n",
      "epoch: 151, train_loss: 0.0684, test_metric: 0.9455, time: 592.7826342582703\n",
      "epoch: 152, train_loss: 0.0676, test_metric: 0.9467, time: 593.0387344360352\n",
      "epoch: 153, train_loss: 0.0658, test_metric: 0.9441, time: 592.7425637245178\n",
      "epoch: 154, train_loss: 0.0655, test_metric: 0.9455, time: 593.6705105304718\n",
      "epoch: 155, train_loss: 0.0662, test_metric: 0.944, time: 592.8283584117889\n",
      "epoch: 156, train_loss: 0.0631, test_metric: 0.9467, time: 593.0684275627136\n",
      "epoch: 157, train_loss: 0.0643, test_metric: 0.9468, time: 592.506854057312\n",
      "epoch: 158, train_loss: 0.0612, test_metric: 0.9439, time: 592.9579458236694\n",
      "epoch: 159, train_loss: 0.0636, test_metric: 0.9421, time: 592.7749092578888\n",
      "epoch: 160, train_loss: 0.0621, test_metric: 0.9435, time: 592.7326045036316\n",
      "epoch: 161, train_loss: 0.0566, test_metric: 0.9469, time: 592.7840070724487\n",
      "epoch: 162, train_loss: 0.0571, test_metric: 0.9451, time: 592.9032351970673\n",
      "epoch: 163, train_loss: 0.0571, test_metric: 0.9442, time: 593.1385943889618\n",
      "epoch: 164, train_loss: 0.056, test_metric: 0.9469, time: 592.870420217514\n",
      "epoch: 165, train_loss: 0.0571, test_metric: 0.9457, time: 592.7501962184906\n",
      "epoch: 166, train_loss: 0.0582, test_metric: 0.9432, time: 592.9322488307953\n",
      "epoch: 167, train_loss: 0.0556, test_metric: 0.9426, time: 592.5645327568054\n",
      "epoch: 168, train_loss: 0.057, test_metric: 0.9461, time: 598.2034902572632\n",
      "epoch: 169, train_loss: 0.0564, test_metric: 0.9457, time: 600.776088476181\n",
      "epoch: 170, train_loss: 0.0537, test_metric: 0.946, time: 599.665735244751\n",
      "epoch: 171, train_loss: 0.0524, test_metric: 0.9457, time: 598.4195716381073\n",
      "epoch: 172, train_loss: 0.0534, test_metric: 0.9473, time: 596.0847623348236\n",
      "epoch: 173, train_loss: 0.0518, test_metric: 0.9464, time: 596.6946604251862\n",
      "epoch: 174, train_loss: 0.0513, test_metric: 0.9445, time: 595.9216110706329\n",
      "epoch: 175, train_loss: 0.0504, test_metric: 0.9462, time: 596.262128829956\n",
      "epoch: 176, train_loss: 0.0494, test_metric: 0.9464, time: 594.937835931778\n",
      "epoch: 177, train_loss: 0.0524, test_metric: 0.9467, time: 594.7331483364105\n",
      "epoch: 178, train_loss: 0.0474, test_metric: 0.9466, time: 592.6097569465637\n",
      "epoch: 179, train_loss: 0.0512, test_metric: 0.9473, time: 592.7780594825745\n",
      "epoch: 180, train_loss: 0.0465, test_metric: 0.9478, time: 592.828724861145\n",
      "epoch: 181, train_loss: 0.0491, test_metric: 0.9484, time: 592.8630402088165\n",
      "epoch: 182, train_loss: 0.0478, test_metric: 0.947, time: 592.9506719112396\n",
      "epoch: 183, train_loss: 0.0469, test_metric: 0.9466, time: 592.7634189128876\n",
      "epoch: 184, train_loss: 0.047, test_metric: 0.9464, time: 593.0373725891113\n",
      "epoch: 185, train_loss: 0.045, test_metric: 0.9496, time: 592.7111806869507\n",
      "epoch: 186, train_loss: 0.0441, test_metric: 0.9489, time: 592.9510650634766\n",
      "epoch: 187, train_loss: 0.0449, test_metric: 0.9473, time: 592.8196630477905\n",
      "epoch: 188, train_loss: 0.0472, test_metric: 0.9478, time: 592.9157712459564\n",
      "epoch: 189, train_loss: 0.0459, test_metric: 0.9476, time: 592.8887782096863\n",
      "epoch: 190, train_loss: 0.0432, test_metric: 0.9478, time: 593.2341561317444\n",
      "epoch: 191, train_loss: 0.0429, test_metric: 0.9487, time: 593.1216235160828\n",
      "epoch: 192, train_loss: 0.0421, test_metric: 0.9452, time: 592.9541523456573\n",
      "epoch: 193, train_loss: 0.0415, test_metric: 0.9496, time: 592.7600653171539\n",
      "epoch: 194, train_loss: 0.0445, test_metric: 0.9486, time: 590.3380465507507\n",
      "epoch: 195, train_loss: 0.0425, test_metric: 0.9488, time: 571.209671497345\n",
      "epoch: 196, train_loss: 0.04, test_metric: 0.9486, time: 573.668696641922\n",
      "epoch: 197, train_loss: 0.0418, test_metric: 0.9505, time: 572.8488976955414\n",
      "epoch: 198, train_loss: 0.0395, test_metric: 0.9497, time: 573.7114977836609\n",
      "epoch: 199, train_loss: 0.0396, test_metric: 0.9496, time: 573.2186880111694\n",
      "epoch: 200, train_loss: 0.0372, test_metric: 0.9502, time: 574.3893473148346\n",
      "epoch: 201, train_loss: 0.0408, test_metric: 0.949, time: 574.8789417743683\n",
      "epoch: 202, train_loss: 0.0406, test_metric: 0.9492, time: 573.4080591201782\n",
      "epoch: 203, train_loss: 0.0408, test_metric: 0.9508, time: 582.7079749107361\n",
      "epoch: 204, train_loss: 0.0399, test_metric: 0.9507, time: 592.7026262283325\n",
      "epoch: 205, train_loss: 0.0397, test_metric: 0.95, time: 592.2263765335083\n",
      "epoch: 206, train_loss: 0.0377, test_metric: 0.9503, time: 592.6051433086395\n",
      "epoch: 207, train_loss: 0.0393, test_metric: 0.9494, time: 592.386744260788\n",
      "epoch: 208, train_loss: 0.0363, test_metric: 0.9502, time: 592.4042534828186\n",
      "epoch: 209, train_loss: 0.0359, test_metric: 0.9516, time: 592.1450462341309\n",
      "epoch: 210, train_loss: 0.0374, test_metric: 0.951, time: 592.0039293766022\n",
      "epoch: 211, train_loss: 0.0373, test_metric: 0.9514, time: 592.2037320137024\n",
      "epoch: 212, train_loss: 0.0339, test_metric: 0.9505, time: 593.8227083683014\n",
      "epoch: 213, train_loss: 0.0384, test_metric: 0.9523, time: 592.8895161151886\n",
      "epoch: 214, train_loss: 0.0344, test_metric: 0.9512, time: 592.983279466629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 215, train_loss: 0.036, test_metric: 0.9517, time: 592.6178703308105\n",
      "epoch: 216, train_loss: 0.0367, test_metric: 0.9502, time: 592.8845608234406\n",
      "epoch: 217, train_loss: 0.0345, test_metric: 0.9503, time: 592.8010311126709\n",
      "epoch: 218, train_loss: 0.0362, test_metric: 0.9524, time: 592.9475555419922\n",
      "epoch: 219, train_loss: 0.0349, test_metric: 0.9512, time: 592.7571353912354\n",
      "epoch: 220, train_loss: 0.0356, test_metric: 0.9507, time: 592.9515450000763\n",
      "epoch: 221, train_loss: 0.0374, test_metric: 0.9514, time: 592.6132092475891\n",
      "epoch: 222, train_loss: 0.0334, test_metric: 0.9501, time: 592.8214802742004\n",
      "epoch: 223, train_loss: 0.0336, test_metric: 0.9512, time: 592.7015759944916\n",
      "epoch: 224, train_loss: 0.0353, test_metric: 0.952, time: 591.5550420284271\n",
      "epoch: 225, train_loss: 0.0337, test_metric: 0.9529, time: 593.0653738975525\n",
      "epoch: 226, train_loss: 0.0335, test_metric: 0.953, time: 593.0131728649139\n",
      "epoch: 227, train_loss: 0.035, test_metric: 0.9519, time: 592.7312452793121\n",
      "epoch: 228, train_loss: 0.0339, test_metric: 0.9513, time: 592.952175617218\n",
      "epoch: 229, train_loss: 0.0308, test_metric: 0.951, time: 592.7062690258026\n",
      "epoch: 230, train_loss: 0.0327, test_metric: 0.9518, time: 593.4824409484863\n",
      "epoch: 231, train_loss: 0.0303, test_metric: 0.9514, time: 594.4780397415161\n",
      "epoch: 232, train_loss: 0.0332, test_metric: 0.951, time: 594.5076682567596\n",
      "epoch: 233, train_loss: 0.034, test_metric: 0.9511, time: 584.6767289638519\n",
      "epoch: 234, train_loss: 0.0336, test_metric: 0.9509, time: 572.0854890346527\n",
      "epoch: 235, train_loss: 0.0334, test_metric: 0.9514, time: 572.4211263656616\n",
      "epoch: 236, train_loss: 0.0304, test_metric: 0.952, time: 572.4746811389923\n",
      "epoch: 237, train_loss: 0.0321, test_metric: 0.9525, time: 572.8053214550018\n",
      "epoch: 238, train_loss: 0.0329, test_metric: 0.9531, time: 573.0169162750244\n",
      "epoch: 239, train_loss: 0.0345, test_metric: 0.9521, time: 573.2441048622131\n",
      "epoch: 240, train_loss: 0.0323, test_metric: 0.9522, time: 573.2391767501831\n",
      "epoch: 241, train_loss: 0.0325, test_metric: 0.9522, time: 573.6877148151398\n",
      "epoch: 242, train_loss: 0.0344, test_metric: 0.952, time: 573.6661512851715\n",
      "epoch: 243, train_loss: 0.0326, test_metric: 0.9517, time: 573.5270295143127\n",
      "epoch: 244, train_loss: 0.0328, test_metric: 0.9523, time: 573.9372985363007\n",
      "epoch: 245, train_loss: 0.0318, test_metric: 0.9516, time: 574.5704848766327\n",
      "epoch: 246, train_loss: 0.0326, test_metric: 0.9518, time: 573.6478507518768\n",
      "epoch: 247, train_loss: 0.0313, test_metric: 0.9516, time: 573.7604715824127\n",
      "epoch: 248, train_loss: 0.0331, test_metric: 0.9525, time: 573.6763412952423\n",
      "epoch: 249, train_loss: 0.032, test_metric: 0.9522, time: 573.5555188655853\n",
      "clean    0.9522\n",
      "fgsm     0.5724\n",
      "pgd      0.3423\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<driver.driver.ABC_Driver at 0x7f9ffdab7f40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = ABC_Driver(cifar10_args, None, record_path=None, if_hash=False)\n",
    "driver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba73e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(driver.model.state_dict(), \"save/CIFAR10_ARC_2023_03_27_09522.pt\")\n",
    "# driver.model.load_state_dict(torch.load(\"save/CIFAR10_ABC_2023_03_07.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
